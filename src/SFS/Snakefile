# !/usr/bin/python
"""
This is snakemake pipeline which will produce posterior range from prior range using msprime SFS and ABC-DLS sequential
sampling method.
"""
import os
import sys
import pandas
from Classes import Misc
from Classes import ABC


##required functions

def narrowing_input(paramsnumbers, inputfile, rangefile, folder=''):
    """
    Narrowing the All.csv file with the range that is calculated by SFS so that it can be used for next cycle

    :param paramsnumbers: the number of parameters in the files. so that the ss part is separated
    :param inputfile: generally all.csv file. whose first few columns are the parameters and last few columns are
        ss
    :param rangefile: the new range in pandas dataframe format. columns should be max and min and indexes should be
            the parameters
    :param folder: to define the output folder. default is '' meaning current folder
    :return: will return the number of lines present in Narrowed.csv file which is created by the function
    """
    newrange = pandas.read_csv(rangefile,header=None,index_col=0).iloc[:, :2]
    newrange.columns = ['min', 'max']
    if Misc.getting_line_count(inputfile) > 0:
        params = pandas.read_csv(inputfile,usecols=range(paramsnumbers),header=None)
        linenumbers = (ABC.ABC_DLS_NS.narrowing_params(params=params,parmin=newrange['min'],parmax=newrange['max'])) - 1
        temp = ABC.ABC_DLS_NS.extracting_by_linenumber(file=inputfile,linenumbers=linenumbers,
            outputfile=folder + 'Narrows.csv')
        if Misc.getting_line_count(temp) > 0:
            _ = ABC.ABC_DLS_NS.shufling_joined_models(inputcsv=temp,output=folder + 'Narrowed.csv',header=False)
            Misc.removefiles([folder + 'Narrows.csv'],printing=False)
        else:
            os.rename(temp,folder + 'Narrowed.csv')
    else:
        with open("Narrowed.csv","w") as my_empty_csv:
            pass
    narrow_count = Misc.getting_line_count(folder + 'Narrowed.csv')
    return narrow_count


def remove_repeated_params(inputfile, paramsnumbers, outputfile):
    """
    remove the repeated parameters lines. We donot need different runs for same simulated parameters

    :param inputfile: generally all.csv file. whose first few columns are the parameters and last few columns are
        ss
    :param paramsnumbers: the number of parameters in the files. so that the ss part is separated
    :param outputfile: the path of the output file
    :return: will return the path of outputfile
    """
    params = pandas.read_csv(inputfile,usecols=range(paramsnumbers),header=None)
    linenumbers = params.drop_duplicates().index.values
    temp = ABC.ABC_DLS_NS.extracting_by_linenumber(file=inputfile,linenumbers=linenumbers,
        outputfile=outputfile)
    return temp


##input info extraction
configfile: 'config.yml'
workdir: os.getcwd()
# extra work

joblist = [format(x,'03') for x in range(1,config["jobs"] + 1)]
config["sfsfile"] = os.path.abspath(config["sfsfile"])
parameteres = pandas.read_csv(config['priors_range'],header=None,names=['par_names', 'upper', 'lower'],
    usecols=[0, 1, 2])
if os.path.isfile('Narrowed.csv'):
    config["current_repeats"] = config["repeats"] - Misc.getting_line_count('Narrowed.csv')
else:
    config["current_repeats"] = config["repeats"]
config["current_repeats"] = max(config["current_repeats"],config["jobs"])
# optional stuff
if "tfknn" in config:
    if not os.path.isfile(config["tfknn"]):
        print("Could not find the tfknn file. If you do not want to use leave it blank as tfknn='' ")
        sys.exit(1)
    tfknn_demo = '--nn ' + os.path.abspath(config["tfknn"])
else:
    tfknn_demo = ''
noise = ''
if 'increase' in config:
    if config["increase"] > 0:
        noise = '--increase ' + str(config["increase"]) + ' --hardrange ' + os.path.abspath(
            config["hardrange_file"])
# real snakemake pipeline starts here
localrules: finalize,info_sfs

rule finalize:
    """
    Finalizing everything and removing unnecessary file. Also counts how many time it ran important if you put the 
    snakemake inside a while loop and you want to count it
    input: output from params_sfs. demography/Newrange.csv. demography/Narrowed.csv updated All_rr.csv
    output: remove demography folder so that we can start a run again. moving important file to current directory 
    All.csv, Newrange.csv and Narrowed.csv
    """
    input:
        config['demography'] + '/Newrange.csv',config['demography'] + '/All_rr.csv',
        config['demography'] + '/Narrowed.csv'
    output:
        'Newrange.csv'
    shell:
        """
        mv {input[0]} ./
        mv {input[1]} ./All.csv 
        mv {input[2]} ./
        rm -fR {config[demography]}
        touch count.out
        awk '{{print $1+1}}' count.out > tmp && mv tmp count.out
        """

rule reusing_simulations:
    """
    to recycle previous run of simulation will produce Narrowed.csv which has Priors+SFS within the new range output. 
    input: All_rr.csv with all the sfs ever ran and the Newrange.csv created by ABC-DLS
    output: Will produce Narrowed.csv which has information of recyclable SFS that was created before
    """
    output:
        config['demography'] + '/Narrowed.csv'
    input:
        config['demography'] + '/All_rr.csv',config['demography'] + '/Newrange.csv'
    params:
        parameteres.shape[0]
    run:
        narrowing_input(paramsnumbers=params[0],rangefile=input[1],inputfile=input[0],folder=config['demography'] + '/')

rule params_sfs:
    """
    Main run for ABC-DLS sequential sampling method. This will using Priors+SFS will give Posterior range which are 
    slightly closer to the real observed data that priors range
    input: output from info_sfs. demography/demography.csv.gz
    output: will produce Newrange.csv which has information of posterior range.  
    demography/Newrange.csv
    """
    input:
        config['demography'] + '/Model.info'
    output:
        config['demography'] + '/Newrange.csv'
    params:
        tfknn_demo=tfknn_demo,
        noise=noise
    shell:
        "python {config[sc_abc]} All --test_size {config[test_size]} --chunksize 1000 --scale b  "
        "--ssfile {config[sfsfile]} --tolerance {config[tolerance]} --method rejection --frac {config[frac]} "
        "{params.tfknn_demo} {params.noise} --folder {config[demography]} --decrease {config[decrease]} {input} "

rule info_sfs:
    """
    Creating Model.info which can be used in ABC-DLS to know the file path and the number of columns designated for 
    parameters.
    input: output from join_SFS_All. demography/demography.csv.gz
    output: tab formatted file path and the parameters columns. demography/Model.info
    """
    input:
        config['demography'] + '/' + config['demography'] + '.csv.gz'
    output:
        config['demography'] + '/Model.info'
    params:
        str(parameteres.shape[0])
    run:
        print(input[0] + '\t' + params[0],file=open(output[0],"w"))

rule all_repeat_remove:
    """
    In case of update there is a chance same line was presented twice (as the new demography.csv.gz has rows from older 
    All.csv. Thus it is better to remove those repeated lines
    input: older All.csv
    output: repeat removed All.csv
    """
    input:
        config['demography'] + '/All.csv'
    output:
        config['demography'] + '/All_rr.csv'
    params:
        int(parameteres.shape[0])
    run:
        remove_repeated_params(inputfile=input[0],paramsnumbers=params[0],outputfile=output[0])

rule updating_all:
    """
    Updating All.csv file which has all the run sfs ran that is done. it will add the newer one that is ran by this
    loop
    input: the older All.csv file
    output: the updated All.csv file
    """
    input:
        'All.csv',config['demography'] + '/' + config['demography'] + '.csv.gz'
    output:
        config['demography'] + '/All.csv'
    shell:
        "cat {input[0]} <(zcat {input[1]}| tail -n+2)  > {output}"

rule join_SFS_All:
    """
    Joining all the different run of sfs and creating one single file. Mainly tackling the header problem from every 
    file to one header for concatenated file. also taking care of simulations from previous loop using Narrowed.csv
    input: output of run_SFS. demography/demography_xxx.csv all
    output: zipped version. demography/demography.csv.gz
    """
    input:
        files=expand(config['demography'] + '/' + config['demography'] + '_{jobint}.csv',jobint=joblist)
    output:
        config['demography'] + '/' + config['demography'] + '.csv.gz'
    params:
        config['repeats'] - config['current_repeats']
    shell:
        "touch Narrowed.csv\n"
        "cat <(head -n1 {input.files[0]}) <(tail -qn+2 {input.files}) <(head -n {params} Narrowed.csv)| grep , "
        "| gzip > {output}"

rule run_SFS:
    """
    The slowest but most important part of the pipeline. Given priors it will produce SFS which then can be used for 
    ABC-DLS. The simulation here is implemented on msprime. 
    input: output of priors_add_header. demography/Priors_xxx.csv
    output: pramaters+sfs together. demography/demography_xxx.csv
    """
    input:
        config['demography'] + '/Priors_{jobint}.csv'
    output:
        config['demography'] + '/' + config['demography'] + '_{jobint}.csv'
    resources:
        mem=6000
    threads: config['threads']
    shell:
        "python {config[sc_sfs]}  --inds {config[inds]} --params_file {input}  --threads {config[threads]} "
        "--total_length {config[total_length]} {config[demography]} > {output}"

rule priors_add_header:
    """
    Split command in the shell do not add the headers. Thus we needed a different approach to add the headers
    input: output of breaks_priors. demography/XXX.csv
    output: just added the header on the input demography/Priors_XXX.csv
    """
    output:
        config['demography'] + '/Priors_{jobint}.csv'
    input:
        config['demography'] + "/{jobint}.csv"
    shell:
        "cat <(head -n 1 {config[demography]}/Priors.csv) <(cat {input}) > {output}"

rule breaks_priors:
    """
    This will break the priors in several files. Useful when we are running in cluster. As the main bottleneck is the 
    SFS generation. Thus separating the priors and independently run them will help parallelization.
    input: coming from priors_create demography.Priors.csv
    output: broken apart Priors.csv demography/XXX.csv assuming we are not going to run more than 1000 jobs together
    """
    output:
        expand(config['demography'] + "/{jobint}.csv",jobint=joblist)
    input:
        config['demography'] + "/Priors.csv"
    shell:
        "tail -n+2 {input} > {config[demography]}/Priors_NH.csv\n"
        "split --numeric-suffixes=1 --number=l/{config[jobs]} -a 3 --additional-suffix=.csv "
        "{config[demography]}/Priors_NH.csv {config[demography]}/\n"

rule priors_create:
    """
    This rule will create Uniform Priors from range. The range should be mentioned in a csv file which then can be load
    by priors_range: from config.yml. 
    input: The range csv should be written in 3 columns. first column is parameter names, second is lower bound of that 
    parameter and third column is the upper bound. The number of current repeats will define how many repeats has to be
    run 
    output: demography/Priors.csv  
    """
    output:
        config['demography'] + "/Priors.csv"
    params:
        par_names=parameteres['par_names'].to_csv(line_terminator=",",index=False,header=False)[:-1],
        upper=parameteres['upper'].to_csv(line_terminator=",",index=False,header=False)[:-1],
        lower=parameteres['lower'].to_csv(line_terminator=",",index=False,header=False)[:-1]
    shell:
        "mkdir -p {config[demography]}\n"
        "python {config[sc_priors]} --par_names {params.par_names} --upper {params.upper} --lower {params.lower} "
        "--repeats {config[current_repeats]} > {output}"
