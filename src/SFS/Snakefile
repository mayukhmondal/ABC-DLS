# !/usr/bin/python
"""
This is snakemake pipeline which will produce posterior range from prior range using msprime SFS and ABC-DLS sequential
sampling method.
"""
import os
import sys

import pandas

from Class import ABC_DLS_SMC_Snakemake
from Classes import Misc

##required functions


##input info extraction
configfile: 'config.yml'
workdir: os.getcwd()
# extra work

joblist = [format(x,'03') for x in range(1,config["jobs"] + 1)]
config["sfsfile"] = os.path.abspath(config["sfsfile"])
parameteres = pandas.read_csv(config['priors_range'],header=None,names=['par_names', 'upper', 'lower'],
    usecols=[0, 1, 2])
def getting_current_repeat(repeats,jobs,narrowed_path="Narrowed.csv"):
    """
    calculating how many repeats have to be done this cycle. The Minimum number is five times of the jobs. as if too less
    number by random chance split will create empty files. It depends on the narrowed_path and repeats as the total
    target is the repeats but some of them already is done by narrowed path which can be reused. Thus remaining will be
    done by this cycle. Little bit more than what is needed is not a problem. Higher the number better for training.
    Lesser the number faster cycle.

    :param repeats: the number of repeats present in config file
    :param jobs: the number of jobs present in config file
    :param narrowed_path: the path of narrowed
    :return: the number of repeats of simulation that is needed in this cycle
    """
    if os.path.isfile(narrowed_path):
        current_repeats = repeats - Misc.getting_line_count(narrowed_path)
    else:
        current_repeats = repeats
    current_repeats = max(current_repeats,jobs*5)
    return current_repeats
config["current_repeats"]=getting_current_repeat(repeats=config["repeats"],jobs=config["jobs"])
# optional stuff
if "tfknn" in config:
    if not os.path.isfile(config["tfknn"]):
        print("Could not find the tfknn file. If you do not want to use leave it blank as tfknn='' ")
        sys.exit(1)
    tfknn_demo = '--nn ' + os.path.abspath(config["tfknn"])
else:
    tfknn_demo = ''
noise = ''
if 'increase' in config:
    if config["increase"] > 0:
        noise = '--increase ' + str(config["increase"]) + ' --hardrange ' + os.path.abspath(
            config["hardrange_file"])
if not 'mutation_rate' in config:
    config['mutation_rate'] = 1.45e-8
if 'ldblock' in config:
    ld = '--ldblock ' + config['ldblock']
else:
    ld = ''
resume_nn=''
if 'resume_nn' in config:
    if config['resume_nn']:
        if os.path.exists(config['demography'] + '/ModelParamPrediction.h5'):
            resume_nn="--resume "+config['demography'] + '/ModelParamPrediction.h5'
            if 'resume_fit' in config:
                if config['resume_fit']:
                    resume_nn = resume_nn+" --resume_fit "+config['resume_fit']
if 'sfs2c' in config:
    sfs2c="--sfs2c"
else:
    sfs2c=""
# real snakemake pipeline starts here
localrules: finalize,info_sfs,calculating_lmrd,priors_add_header

rule finalize:
    """
    Finalizing everything and removing unnecessary file. Also add the log of mean range decrease (lmrd) in lmrd.out 
    file. So you can see how the lmrd evolves per cycle. 
    input: output from params_sfs. demography/Newrange.csv. demography/Narrowed.csv updated All_rr.csv 
    demography/lmrd.out
    output: remove demography folder so that we can start a run again. moving important file to current directory 
    All.csv, Newrange.csv and Narrowed.csv adding new lmrd in lmrd.out
    """
    input:
        config['demography'] + '/Newrange.csv',config['demography'] + '/All_rr.csv',
        config['demography'] + '/Narrowed.csv',config['demography'] + '/lmrd.out'
    output:
        'Newrange.csv'
    shell:
        """
        mv {input[0]} ./
        mv {input[1]} ./All.csv 
        mv {input[2]} ./
        touch lmrd.out
        cat {input[3]} >> lmrd.out        
        """

rule calculating_lmrd:
    """
    calculating log of mean range decrease (lmrd) for the newrange over hardrange. 
    input: Newrange.csv which has information of posterior range of this cycle and hardrage.csv which has hardrange 
    information
    output: will create lmrd.out in demography folder and print lmrd there
    """
    input:
        config['demography'] + '/Newrange.csv',config["hardrange_file"]
    output:
        temp(config['demography'] + '/lmrd.out')
    run:
        if "hardrange_file" in config:
            lmrd = ABC_DLS_SMC_Snakemake.lmrd4mcsv(hardrange_file=config["hardrange_file"],newrange_file=input[0])
        else:
            lmrd = 0
        with open(output[0],'w') as f:
            print(lmrd,file=f)

rule reusing_simulations:
    """
    to recycle previous run of simulation will produce Narrowed.csv which has Priors+SFS within the new range output. 
    input: All_rr.csv with all the sfs ever ran and the Newrange.csv created by ABC-DLS
    output: Will produce Narrowed.csv which has information of recyclable SFS that was created before
    """
    output:
        temp(config['demography'] + '/Narrowed.csv')
    input:
        config['demography'] + '/All_rr.csv',config['demography'] + '/Newrange.csv'
    params:
        parameteres.shape[0]
    run:
        ABC_DLS_SMC_Snakemake.narrowing_input(paramsnumbers=params[0],rangefile=input[1],inputfile=input[0],
            folder=config['demography'] + '/')

rule params_sfs:
    """
    Main run for ABC-DLS SMC sampling method. This will using Priors+SFS will give Posterior range which are 
    slightly closer to the real observed data that priors range
    input: output from info_sfs. demography/demography.csv.gz
    output: will produce Newrange.csv which has information of posterior range.  
    demography/Newrange.csv
    """
    input:
        info=config['demography'] + '/Model.info',
        csv=config['demography'] + '/' + config['demography'] + '.csv.gz',
        sfs=config['sfsfile']
    output:
        temp(config['demography'] + '/Newrange.csv'), temp(config['demography'] + '/scale_x.sav'),
        temp(config['demography'] + '/scale_y.sav'), temp(config['demography'] + '/x.h5'),
        temp(config['demography'] + '/y.h5'),temp(config['demography'] + '/params_header.csv')
    params:
        tfknn_demo=tfknn_demo,
        noise=noise,
        resume_nn=resume_nn
    resources:
        mem_mb=6000
    shell:
        "python {config[sc_abc]} All --test_size {config[test_size]} --chunksize 1000 --scale b  "
        "--ssfile {config[sfsfile]} --tolerance {config[tolerance]} --method rejection --frac {config[frac]} "
        "{params.tfknn_demo} {params.noise} --folder {config[demography]} --decrease {config[decrease]} {input[info]} "
        "{params[resume_nn]}"

rule info_sfs:
    """
    Creating Model.info which can be used in ABC-DLS to know the file path and the number of columns designated for 
    parameters.
    input: output from join_SFS_All. demography/demography.csv.gz
    output: tab formatted file path and the parameters columns. demography/Model.info
    """
    input:
        config['demography'] + '/' + config['demography'] + '.csv.gz'
    output:
        temp(config['demography'] + '/Model.info')
    params:
        str(parameteres.shape[0])
    run:
        print(input[0] + '\t' + params[0],file=open(output[0],"w"))

rule all_repeat_remove:
    """
    In case of update there is a chance same line was presented twice (as the new demography.csv.gz has rows from older 
    All.csv. Thus it is better to remove those repeated lines
    input: older All.csv
    output: repeat removed All.csv
    """
    input:
        config['demography'] + '/All.csv'
    output:
        temp(config['demography'] + '/All_rr.csv')
    params:
        int(parameteres.shape[0])
    run:
        ABC_DLS_SMC_Snakemake.remove_repeated_params(inputfile=input[0],paramsnumbers=params[0],outputfile=output[0])

rule updating_all:
    """
    Updating All.csv file which has all the run sfs ran that is done. it will add the newer one that is ran by this
    loop
    input: the older All.csv file
    output: the updated All.csv file
    """
    input:
        'All.csv',config['demography'] + '/' + config['demography'] + '.csv.gz'
    output:
        temp(config['demography'] + '/All.csv')
    shell:
        "cat {input[0]} <(zcat {input[1]}| tail -n+2)  > {output}"

rule join_SFS_All:
    """
    Joining all the different run of sfs and creating one single file. Mainly tackling the header problem from every 
    file to one header for concatenated file. also taking care of simulations from previous loop using Narrowed.csv
    input: output of run_SFS. demography/demography_xxx.csv all
    output: zipped version. demography/demography.csv.gz
    """
    input:
        files=expand(config['demography'] + '/' + config['demography'] + '_{jobint}.csv',jobint=joblist)
    output:
        temp(config['demography'] + '/' + config['demography'] + '.csv.gz')
    params:
        config['repeats'] - config['current_repeats']
    shell:
        "touch Narrowed.csv\n"
        "cat <(head -n1 {input.files[0]}) <(tail -qn+2 {input.files}) <(head -n {params} Narrowed.csv)| grep , "
        "| gzip > {output}"

rule run_SFS:
    """
    The slowest but most important part of the pipeline. Given priors it will produce SFS which then can be used for 
    ABC-DLS. The simulation here is implemented on msprime. 
    input: output of priors_add_header. demography/Priors_xxx.csv
    output: pramaters+sfs together. demography/demography_xxx.csv
    """
    input:
        config['demography'] + '/Priors_{jobint}.csv'
    output:
        temp(config['demography'] + '/' + config['demography'] + '_{jobint}.csv')
    params:
        ld=ld,
        sfs2c=sfs2c
    resources:
        mem_mb=6000
    threads: config['threads']
    shell:
        "python {config[sc_sfs]}  --inds {config[inds]} --params_file {input}  --threads {config[threads]} "
        "--total_length {config[total_length]} --mutation_rate {config[mutation_rate]} {config[demography]} {params.ld}"
        " {sfs2c} > {output}"

rule priors_add_header:
    """
    Split command in the shell do not add the headers. Thus we needed a different approach to add the headers
    input: output of breaks_priors. demography/XXX.csv
    output: just added the header on the input demography/Priors_XXX.csv
    """
    output:
        temp(config['demography'] + '/Priors_{jobint}.csv')
    input:
        prior=config['demography'] + "/{jobint}.csv",
        header=config['demography']+ "/Priors.csv"
    shell:
        "cat <(head -n 1 {input[header]}) <(cat {input[prior]}) > {output}"

rule breaks_priors:
    """
    This will break the priors in several files. Useful when we are running in cluster. As the main bottleneck is the 
    SFS generation. Thus separating the priors and independently run them will help parallelization.
    input: coming from priors_create demography.Priors.csv
    output: broken apart Priors.csv demography/XXX.csv assuming we are not going to run more than 1000 jobs together
    """
    output:
        temp(expand(config['demography'] + "/{jobint}.csv",jobint=joblist)),
        temp(config['demography']+'/Priors_NH.csv')
    input:
        config['demography'] + "/Priors.csv"
    shell:
        "tail -n+2 {input} > {config[demography]}/Priors_NH.csv\n"
        "split --numeric-suffixes=1 --number=l/{config[jobs]} -a 3 --additional-suffix=.csv "
        "{config[demography]}/Priors_NH.csv {config[demography]}/\n"

rule priors_create:
    """
    This rule will create Uniform Priors from range. The range should be mentioned in a csv file which then can be load
    by priors_range: from config.yml. 
    input: The range csv should be written in 3 columns. first column is parameter names, second is lower bound of that 
    parameter and third column is the upper bound. The number of current repeats will define how many repeats has to be
    run 
    output: demography/Priors.csv  
    """
    output:
        temp(config['demography'] + "/Priors.csv")
    params:
        par_names=parameteres['par_names'].to_csv(line_terminator=",",index=False,header=False)[:-1],
        upper=parameteres['upper'].to_csv(line_terminator=",",index=False,header=False)[:-1],
        lower=parameteres['lower'].to_csv(line_terminator=",",index=False,header=False)[:-1]
    shell:
        "mkdir -p {config[demography]}\n"
        "python {config[sc_priors]} --par_names {params.par_names} --upper {params.upper} --lower {params.lower} "
        "--repeats {config[current_repeats]} > {output}"
